{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pprint, time\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1601564351868,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "lXyR3B1-cst0"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 991,
     "status": "ok",
     "timestamp": 1601564351868,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "McTkC40Tcst3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = [6,6]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Participants: Alvaro Viejo (100451677), Rodrigo Oliver (100451788), Oscar Montoya (100451858), Miguel Ángel Ponce (100451309)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8pLlMwKcst8"
   },
   "source": [
    "## Degree in Data Science and Engineering, group 96\n",
    "## Machine Learning 2\n",
    "### Fall 2020\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "# Lab 8. Hidden Markov Models\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "**Ignacio Peis**\n",
    "\n",
    "Dept. of Signal Processing and Communications\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.uc3m.es/ss/Satellite?blobcol=urldata&blobkey=id&blobtable=MungoBlobs&blobwhere=1371573953235&ssbinary=true' width=400 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "**Markov Models** are one of the simplest ways to treat sequential data, by relaxing the i.i.d. (independent and identically distributed) assumption. The joint distribution for a sequence of observations is given by:\n",
    "$$ p(\\mathbf{x}_1, \\mathbf{x}_2, ..., \\mathbf{x}_T) = p(\\mathbf{x}_{1:T}) = \\prod_{t=2}^T p(\\mathbf{x}_t | \\mathbf{x}_1, ..., \\mathbf{x}_{t-1}) $$\n",
    "where we assume that each observation depends on all previous observations, which can be expressed by means of the product rule.\n",
    "This expression can be relaxed to obtain simpler models by establishing dependencies only with the previous observation (*first-order Markov chain*) or the two previous ones (*second order Markov chain*).\n",
    "\n",
    "Suppose we wish to build a model for sequences that is not limited by the Markov assumption to any order and yet that can be specified using a limited number of free parameters. We can achieve this by introducing additional latent variables to permit a rich class of models to be constructed out ot simple components, as we did with mixture distributions. For each observation $\\textbf{x}_t$ , we introduce a corresponding latent variable $\\textbf{z}_t$, which might be of different type or dimensionality to the observed variable. The joint distribution for this model is given by:\n",
    "$$ p(\\mathbf{x}_{1:T}, \\mathbf{z}_{1:T}) = p(\\mathbf{z}_1 ) \\left[ \\prod_{t=2}^T p(\\mathbf{z}_t|\\mathbf{z}_{t-1}  ) \\right] \\prod_{t=1}^T p(\\mathbf{x}_t |\\mathbf{z}_t) $$\n",
    "If this latent variable is continous, we call the model a *state space model*. In contrast, if the latent variable is discrete, we have a **Hidden Markov Model** and is denoted as **state**, $\\mathbf{z}_t \\equiv s_t$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Hidden Markov Models\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~ipeis/ML2/hmm.png' width=400 />\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "A Hidden Markov Model (HMM) consists of a discrete-time, discrete-state Markov chain (first-order Markov discrete model), with hidden states $s_t \\in \\{ 1, ..., L \\}$, plus an **observation** model $p(\\mathbf{y}_t | s_t)$. The joint distribution has the form:\n",
    "$$ p(\\mathbf{y}_{1:T}, s_{1:T}) = p(s_1 ) \\left[ \\prod_{t=2}^T p(s_t|s_{t-1}  ) \\right] \\prod_{t=1}^T p(\\mathbf{y}_t |s_t) $$\n",
    "The probabilities of each observation $p(\\mathbf{y}_t |s_t)$ given the state is called **emission**. If the data is continuous, it follows a Gaussian distribution:\n",
    "$$ p(\\mathbf{y}_t |s_t) = \\mathcal{N} (\\mathbf{y}_t | \\boldsymbol{\\mu}_i, \\boldsymbol{\\Sigma}_i) $$\n",
    "Hence, for each observation $\\mathbf{y}_t$, we have a set of $L$ possible Gaussians, depending on the state $s_t$ that generated the sample. In other terms, a HMM can be seen as a sequential GMM, where the component of each sample depends on the previous one.\n",
    "The probabilities of the discrete hidden states are defined by the transition matrix, $\\mathbf{A}$. Each of its elements $a_{ij}$ defines the probability of state $j$ given that the previous state was $i$. Further, we need a probability for the initial states (as they do not have previous states). This probabilities are called $\\boldsymbol{\\pi}$.\n",
    "\n",
    "- $ S = \\{ s_1, s_2, ..., s_T : s_t \\in 1, ..., L \\}  $: hidden state sequence.\n",
    "- $ Y = \\{ \\mathbf{y}_1, \\mathbf{y}_2, ..., \\mathbf{y}_T : \\mathbf{y}_t \\in \\mathbb{R}^M \\}  $: observed continuous sequence.\n",
    "- $ \\mathbf{A} = \\{ a_{ij}: a_{ij} = p(s_{t+1}=j | s_t = i \\} $: state transition probabilities.\n",
    "- $ \\mathbf{B} = \\{ b_{i}: p_{b_i}(\\mathbf{y}_t) = p(\\mathbf{y}_t | s_t = i \\} $: observation emission probabilities.\n",
    "- $ \\boldsymbol{\\pi} = \\{ \\pi_i: \\pi_i = p(s_1=i) \\} $: initial state probability distribution.\n",
    "- $ \\boldsymbol{\\theta} = \\{ \\mathbf{A}, \\mathbf{B}, \\boldsymbol{\\pi} \\} $: model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Inference in HMMs\n",
    "\n",
    "In graphical models, given the joint distribution, we can perform probabilistic inference. This refers to the task of estimating unknown quantities from known quantities. i.e., the distributions for latent variables given the observations. Provided the HMM model, several inference scenarios might be needed to be solved.\n",
    "\n",
    "### 2.1.1. The Forwards Algorithm\n",
    "\n",
    "The forwards algorithm is used with the aim at obtaining the evidence of a sequence, given the sequence $Y$ and the parameters $\\theta$: \n",
    "$$p(Y | \\theta) = \\sum_S p(Y, S | \\theta)$$\n",
    "As $S$ is hidden (we do not observe it), we have to infer its probabilities. This is solved by means of the **Forwards algorithm**, which compute the filtered marginals $p(s_t=j | \\mathbf{y}_{1:t})$ recursively:\n",
    "$$ \\alpha_t(i) = p(s_t=i | \\mathbf{y}_{1:t}) $$\n",
    "\n",
    "$$ \\alpha_1(i) = \\pi_i p_{b_i}(\\mathbf{y}_1) $$\n",
    "$$ \\alpha_t(i) = \\left( \\sum_{j=1}^L \\alpha_{t-1}(j) a_{ji} \\right) p_{b_i}(\\mathbf{y}_t) $$\n",
    "\n",
    "### 2.1.2. The Forwards-backwards Algorithm\n",
    "\n",
    "The goal is to estimate the probability for state $s_t$ at time $t$, given a sequence of observations $Y$ and the parameters of the model $\\theta$. For that purpose, we compute the smoothed marginals, $p(s_t=j | \\mathbf{y}_{1:T})$ using offline inference (we know future observations for each time $t$):\n",
    "$$ \\gamma_t(j) = p(s_t | y_{1:T}) = \\alpha_t(j) \\beta_t(j) $$\n",
    "where:\n",
    "$$ \\beta_T(i) = 1 $$\n",
    "$$ \\beta_t(i) = \\sum_j a_{ij} P_{b_j} (\\mathbf{y}_{t+1}) \\beta_{t+1}(j)  $$\n",
    "\n",
    "### 2.1.3. The Viterbi Algorithm\n",
    "\n",
    "\n",
    "In this case, the objective is obtaining the optimal entire sequence of states given the sequence of observations $Y$ and the parameters $\\theta$:\n",
    "$$ S^* = \\underset{S}{argmax} \\, p(S |Y, \\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Learning in HMMs: EM by The Baum-Welch algorithm\n",
    "\n",
    "In graphical models, we denote as *learning* the computation of the parameters that explain a given set of observations. In HMMs, the parameters $\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\pi}, \\textbf{A}, \\textbf{B} \\} $ can be obtained by using a special version of the EM algorithm, The Baum-Welch algorithm. The process is the same that for GMMs, with the difference that now we consider the time dependencies. \n",
    "\n",
    "### 2.2.1. E-step\n",
    "The expected complete data log likelihood is given by:\n",
    "\n",
    "$$ Q(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}^{old}) = \\sum_{i=1}^L \\left( \\mathbb{E}[N_i^1] \\right) \\log{\\pi_i} + \\\\ \n",
    "\\sum_{i=1}^L \\sum_{j=1}^L \\mathbb{E} [N_{ij}] \\log{a_{ij}} + \\\\\n",
    "\\sum_{n=1}^N \\sum_{t=1}^{T_n} \\sum_{i=1}^L p(s_t=i | \\mathbf{y}_t, \\boldsymbol{\\theta}^{old}) \\log{p(\\mathbf{y}_{n, t} | b_i)}\n",
    "$$\n",
    "\n",
    "where the expected counts are given by:\n",
    "$$ \\mathbb{E}[N_i^1] = \\sum_{n=1}^N p(s_{n1}=i | \\mathbf{y}_n, \\boldsymbol{\\theta}^{old}) = \\sum_{n=1}^N \\gamma_{n, 1}(i) \\\\\n",
    "\\mathbb{E}[N_{ij}] = \\sum_{n=1}^N \\sum_{t=2}^{T_n} p(s_{n,t-1} = i, s_{n,t} = j | \\mathbf{y}_n, \\boldsymbol{\\theta}^{old}))  = \\sum_{n=1}^N \\xi_{n,t}(i,j) \\\\\n",
    "\\mathbb{E}[N_i] = \\sum_{n=1}^N \\sum_{t=1}^{T_n} p(s_{n,t} = i | s_{n,t} = j | \\mathbf{y}_n, \\boldsymbol{\\theta}^{old}))\n",
    "$$\n",
    "\n",
    "This function can be expressed in terms of the **smoothed node and edge marginals**:\n",
    "$$ \\gamma_{n, t}(i) = p(s_t=i | \\mathbf{y}_{n, 1:T_n}, \\boldsymbol{\\theta}) \\\\\n",
    "\\xi_{n, t}(i, j)  = p(s_{t-1} = i, s_t=j | \\mathbf{y}_{n, 1:T_n}, \\boldsymbol{\\theta}) = \\alpha_t(i) a_{ij} P_{b_j}(\\mathbf{y}_{t+1}) \\beta_{t+1}(j)\n",
    "$$\n",
    "\n",
    "These are the two variables that must be computed during the E-step, as we did with the *responsibilities* in the EM algorithm for GMMs.\n",
    "\n",
    "### 2.2.2. M-step\n",
    "Given all these computed variables, we can update the parameters $\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\pi}, \\textbf{A}, \\textbf{B} \\} $ using:\n",
    "$$ \\hat{\\pi}_i= \\frac{\\mathbb{E}[N_i^1]}{N} $$\n",
    "$$ \\hat{a}_{ij}= \\frac{\\mathbb{E}[N_{ij}]}{\\sum_{j'} \\mathbb{E}[N_{ij'}]} $$\n",
    "The emission model will depend on the type of the data. For a Gaussian emission, the parameters for each component will be:\n",
    "$$ \\hat{\\boldsymbol{\\mu}}_i = \\frac{\\mathbb{E}[\\bar{\\mathbf{y}}_i]}{\\mathbb{E}[N_i]} \\qquad \n",
    "\\hat{\\boldsymbol{\\Sigma}}_i = \\frac{ \\mathbb{E}[(\\overline{yy})_i^T] - \\mathbb{E}[N_i]\\hat{\\boldsymbol{\\mu}}_i \\hat{\\boldsymbol{\\mu}}_i^T }{ \\mathbb{E}[N_i] }\n",
    "$$ \n",
    "where the expected sufficient statistics are\n",
    "$$ \\mathbb{E}[\\bar{y}_i] = \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\gamma_{n,t}(i) \\mathbf{y}_{n,t} $$\n",
    "$$ \\mathbb{E}[\\overline{yy}_i^T] = \\sum_{n=1}^N \\sum_{t=1}^{T_n} \\gamma_{n,t}(i) \\mathbf{y}_{n,t} \\mathbf{y}_{n,t}^T $$\n",
    "\n",
    "By the other hands, if the observations are discrete, the emissions follow a Mutinouilli model, and the parameters are a matrix $\\mathbf{B}$ where:\n",
    "$$ \\hat{b}_{im} = \\frac{\\mathbb{E} [M_{im}] }{\\mathbb{E} [N_{i}]}$$\n",
    "where:\n",
    "$$ \\mathbb{E} [M_{im}] = \\sum_{n=1}^{N} \\sum_{t=1}^{T_n} \\gamma_{n,t}(i) \\mathbb{I}(y_{n,t}=m) = \\sum_{n=1}^{N} \\sum_{t:y_{n, t}=l} \\gamma_{n,t} (i)$$\n",
    "and $m$ correspond to the discrete observed varaible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. HMMs with fully observed data\n",
    "\n",
    "There is another approach for HMMs where we assume that the states are also observed. In this case, inference of the states is not required, as we actually know the values of $S$. We can directly compute the Maximum Likelihood Estimation for $\\mathbf{A}$ and $\\boldsymbol{\\pi}$ with the expressions:\n",
    "\n",
    "$$ \\hat{a}_{ij}= \\frac{\\mathbb{E}[N_{ij}]}{\\sum_{j'} \\mathbb{E}[N_{ij'}]} $$\n",
    "$$ \\hat{\\pi}_i= \\frac{\\mathbb{E}[N_i^1]}{N} $$\n",
    "\n",
    "The transitions $\\hat{a}_{ij}$ are obtained as the proportion of observed transitions from state $i$ to $j$ with respect to all the states $i$. The $\\hat{\\pi}_i$ initial probabilities are simply the proportion of states $i$ with respect to all the observed states. \n",
    "\n",
    "In the second part of this notebook we will fit a HMM with fully observed discrete data. For the discrete case, the emissions are given by:\n",
    "$$ \\hat{b}_{im} = \\frac{N_{im}^X}{N_i} \\qquad N_{im}^X = \\sum_{n=1}^{N} \\sum_{t=1}^{T_n}  \\mathbb{I}(s_{n,t}=i, x_{n,t}=m)$$\n",
    "The emission $\\hat{b}_{im}$ is basically the proportion of times a word $m$ appears with tag $i$ with respect to all the words with tag $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. HMMs in hmmlearn\n",
    "\n",
    "[hmmlearn](https://hmmlearn.readthedocs.io/en/latest/api.html) is a python package that implements the HMM model and its principal applications of inference and learning, and follows scikit-learn API as close as possible, but adapted to sequence data. Its easy interface allows to create HMM models and optimize their parameters using the Baum-Welch algorithm in a few lines of code.\n",
    "\n",
    "An example of the definition of a HMM, and sampling sequences from the model, is included below. For this model, observations $\\textbf{y}_t$ are bidimensional, and we use $L=3$ states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Number of states\n",
    "L=3\n",
    "\n",
    "# Define the HMM model\n",
    "model = hmm.GaussianHMM(n_components=L, covariance_type=\"full\")\n",
    "\n",
    "# SETTING THE PARAMETERS\n",
    "# Prior probabilities Pi\n",
    "Pi = np.array([0.6, 0.3, 0.1])\n",
    "model.startprob_ = Pi\n",
    "\n",
    "# Transition matrix A\n",
    "A = np.array([[0.7, 0.2, 0.1],\n",
    "              [0.3, 0.5, 0.2],\n",
    "              [0.3, 0.3, 0.4]])\n",
    "model.transmat_ = A\n",
    "\n",
    "# Parameters of the state-condition Gaussian densities p(yt|st)\n",
    "mus = np.array([[0.0, 0.0], [3.0, -3.0], [5.0, 10.0]])\n",
    "Sigmas = np.tile(np.identity(2), (3, 1, 1))\n",
    "model.means_ = mus\n",
    "model.covars_ = Sigmas\n",
    "\n",
    "# Obtain a sequence of 100 samples\n",
    "Y, S = model.sample(100)\n",
    "\n",
    "f, ax = plt.subplots(2, 1, figsize=(12, 8), sharex=True, gridspec_kw={'height_ratios': [2, 1]})\n",
    "ax[0].plot(Y[:, 0], label=r'$y_0$')\n",
    "ax[0].plot(Y[:, 1], label=r'$y_1$')\n",
    "ax[0].plot(model.means_[S][:, 0], ':', color='tab:blue', label=r'$\\mu_{k0}$')\n",
    "ax[0].plot(model.means_[S][:, 1], ':', color='tab:orange', label=r'$\\mu_{k1}$')\n",
    "ax[0].grid(alpha=0.4)\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_title('samples')\n",
    "ax[1].plot(S, '-o', color='tab:red')\n",
    "plt.yticks([0, 1, 2], ['State 0', 'State 1', 'State 2'])\n",
    "ax[1].grid(alpha=0.4)\n",
    "ax[1].set_title('states')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Experiments\n",
    "\n",
    "## 3.1. Human Activity Recognition (HAR)\n",
    "\n",
    "In Human Activity Recognition, the states $s_t$ can be used to represent activities or gestures and the observations $\\mathbf{y}_t$ to features extracted from video or sensors signals. Each activity defines a distribution for the inertial sensors. The signals of an accelerometer will change depending on whether the patient is lying or running, for example.\n",
    "\n",
    "The [DaLiAc (Daily Life Activities) database](https://www.mad.tf.fau.de/research/activitynet/daliac-daily-life-activities/) consists of data from 19 subjects (8 female and 11 male, age 26 ± 8 years, height 177 ± 11 cm, weight 75.2 ± 14.2 kg, mean ± standard deviation (SD)) that performed 13 daily life activities.\n",
    "\n",
    "Four sensors were used for data acquisition. Each sensor node was equipped with a triaxial accelerometer and a triaxial gyroscope. Data were sampled with 204.8 Hz Hz and were stored on SD card. The sensor nodes were placed on the left ankle, the right hip, the chest, and the right ankle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('HAR_data.csv', index_col=0)\n",
    "activities = ['Sitting', 'Lying', 'Standing', 'Washing Dishes', 'Vacuuming', 'Sweeping', 'Walking',\n",
    "              'Ascending stairs', 'Descending stairs', 'Treadmill running', \n",
    "             'Bicycling on ergometer (50W)', 'Bicycling on ergometer (100W)', 'Rope jumping']\n",
    "labels = data.iloc[:, -1].values\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 204.8\n",
    "f, ax = plt.subplots(1, 2, figsize=(16,4), sharey=True)\n",
    "t = np.arange(data.shape[0])/fs / 60\n",
    "ax[0].plot(t,  data.iloc[:, 0:3])\n",
    "ax[0].set_xlabel('min')\n",
    "ax[0].grid(alpha=0.4)\n",
    "ax[0].set_title('Whole sequence')\n",
    "ax[0].set_ylabel(r'$m/s^2$')\n",
    "ax[1].plot(t[:300]*60, data.iloc[:300, 0:3])\n",
    "ax[1].set_xlabel('sec')\n",
    "ax[1].grid(alpha=0.4)\n",
    "ax[1].set_title('Segment with 300 observations')\n",
    "plt.suptitle('Accelerometer data')\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(16,4), sharey=True)\n",
    "ax[0].plot(np.arange(data.shape[0])/fs / 60,  data.iloc[:, 3:6])\n",
    "ax[0].set_xlabel('min')\n",
    "ax[0].grid(alpha=0.4)\n",
    "ax[0].set_title('Whole sequence')\n",
    "ax[0].set_ylabel(r'$deg/s$')\n",
    "ax[1].plot(t[:300]*60, data.iloc[:300, 3:6])\n",
    "ax[1].set_xlabel('sec')\n",
    "ax[1].grid(alpha=0.4)\n",
    "ax[1].set_title('Segment with 300 observations')\n",
    "plt.suptitle('Gyroscope data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Data Preprocessing\n",
    "\n",
    "The sampling frequency is $f_s=204.8 Hz$. As it has no sense to guess the performed activity each $1/f_s=4.8 ms$, in HAR, segmentation of the data is performed using windows of $W$ samples. Thus, a segment of 5 seconds correspond to a window of 1024 samples. For each window, we extract the mean and standard deviation as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "data_n = data.iloc[:, :-2]\n",
    "scaler = StandardScaler().fit(data_n)\n",
    "data_n = scaler.transform(data_n)\n",
    "\n",
    "\n",
    "fs = 204.8  # 204.8 Hz\n",
    "Twind = 5 # 5 seconds\n",
    "Wwind = int(fs * Twind)\n",
    "nsegments = np.ceil(len(data_n) / Wwind).astype(int)\n",
    "\n",
    "X = []\n",
    "acts = []\n",
    "for s in range(nsegments):\n",
    "    # Extract features for each segment\n",
    "    segment = data_n[s*Wwind:(s+1)*Wwind, :]\n",
    "    mean = np.mean(segment, axis=0)\n",
    "    std = np.std(segment, axis=0)\n",
    "    feat = np.concatenate((mean, std))\n",
    "    X.append(feat)\n",
    "    \n",
    "    # Labels\n",
    "    l = labels[s*Wwind:(s+1)*Wwind]\n",
    "    counts = np.unique(l, return_counts=True)\n",
    "    label = counts[0][np.argmax(counts[1])]\n",
    "    acts.append(label)\n",
    "\n",
    "X = np.stack(X)\n",
    "acts = np.stack(acts) # range 0:K-1\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(16,4), sharey=True)\n",
    "t = np.arange(X.shape[0])*Wwind/fs / 60\n",
    "ax[0].plot(t, X[:, :3])\n",
    "ax[0].set_xlabel('min')\n",
    "ax[0].grid(alpha=0.4)\n",
    "ax[0].set_title('Preprocessed accelerometer')\n",
    "ax[1].plot(t, X[:, 6:9])\n",
    "ax[1].set_xlabel('min')\n",
    "ax[1].grid(alpha=0.4)\n",
    "ax[1].set_title('Preprocessed gyroscope');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Learning the parameters of the HMM\n",
    "\n",
    "**TASK1: Using hmmlearn, create a Gaussian HMM model and fit it to the preprocessed data stored in $X$ from the DaLiAc database. Using plt.imshow(), show the transition matrix $A$ as an image, in order to visualize better its elements. Use plt.xticks and plt.yticks to include the activities as labels.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4396)\n",
    "model = hmm.GaussianHMM(n_components=len(activities), covariance_type=\"full\")\n",
    "model.fit(X)\n",
    "\n",
    "#Transition Matrix\n",
    "A= model.transmat_\n",
    "\n",
    "plt.imshow(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Q1. Decribe the transition matrix you have obtained, and provide with your intuitions about it.**\n",
    "- The transition matrix we obtained is clearly pointing to return to the same activity with a much higher probability than changing to another. Therefore our model is correctly associating a state with an activity. Without knowing the activity because the states are random.\n",
    "\n",
    "**Q2. Is there any state for which the probability of transitioning to other state is considerable? Why?**\n",
    "- No, because the same as we said before our model seems to predict properly at least that a state is corresponding to only one activity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Inference in HMM: MAP estimation of the states.\n",
    "\n",
    "**TASK2: Use the corresponding method of hmmlearn to run the Viterbi algorithm that gives you the most probable sequence of states given the signal $X$. Plot the sequence of activities and the decoded states of the HMM. Use the given list of activities for the yticks in the plot.**\n",
    "\n",
    "*Note: the states of the HMM does not have to be aligned with the labels. For example, state 4 might be associated to activity 1. Use the provided function to align the states with the activities.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def align_states(acts, states, activities=activities):\n",
    "    \"\"\"Align each hidden state with its corresponding class.\"\"\"\n",
    "    \n",
    "    # M_ci show how many points of class c has been associated to state i\n",
    "    L = len(activities)\n",
    "    M = confusion_matrix(acts, states)\n",
    "    plt.figure()\n",
    "    plt.imshow(M)\n",
    "    plt.xticks(np.arange(L), ['state '+str(i) for i in range(L)], rotation=90)\n",
    "    plt.yticks(np.arange(L), activities)\n",
    "    align = np.argmax(M, axis=0)\n",
    "    aligned_states = states.copy()\n",
    "    for s in range(13):\n",
    "        aligned_states[states==s] = align[s]\n",
    "    return aligned_states\n",
    "\n",
    "states = model.predict(X)\n",
    "aligned = align_states(acts, states, activities=activities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Q3. Why the state index does not coincide with the class index?**\n",
    "- Because the class index is hidden and we do not know which activity is each state, that is the problem for the algorithim to obtain the higher probs and assign a possible class for each state.\n",
    "  \n",
    "**Q4. Is there any set of activities that the model hardly distinguishes?**\n",
    "- Clearly walking is related to state 1 with a high probability while for the remaining activities there is not a clear distinguishable state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Part-Of-Speech (POS) Tagging with Hidden Markov Models\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~ipeis/ML2/POS.png' width=500 />\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "In Natural Language Processing (NLP), associating each word in a sentence with a proper POS (part of speech) is known as POS tagging or POS annotation. POS tags are also known as word classes, morphological classes, or lexical tags. A HMM can be fitted to text data, using the states $S$ for modeling the tags. Hence, for this part, we will fit a **HMM with fully observed data**.\n",
    "\n",
    "In this experiment, we will use a Multinomial HMM. Features are the appearance of each word in a vocabulary conformed by words from a big database of sentences. Hence, the emission parameters are a big matrix $\\mathbf{B}$ with dimensions $L\\times M$, where $L$ is the number of states (number of tags) and $M$ is the size of the vocabulary (number of different words in the database).\n",
    "\n",
    "The dataset for this experiment is the Treebank database, that can be easily download from the [nltk package for NLP in python](https://www.nltk.org/). As the dataset is tagged, we do not have to perform inference, we can just obtain the model parameters by applying the \"Fully Observed Data\" expressions of section 2.3. The dataset consist on a total of 3914 sentences, and each sentence $n$ is composed by $T_n$ words with their respective tags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk datasets will be downloaded in \"Users/user/nltk_data\".\n",
    "\n",
    "#download the treebank corpus from nltk\n",
    "nltk.download('treebank')\n",
    " \n",
    "#download the universal tagset from nltk\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "# reading the Treebank tagged sentences\n",
    "# data contains a list with sentences\n",
    "# each sentence is a list of tuples with a word and the corresponding tag\n",
    "data = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))\n",
    "print('Number of sentences: ' + str(len(data)))\n",
    "\n",
    "#print each word with its respective tag for first two sentences\n",
    "print('First two sentences:')\n",
    "for sent in data[:2]:\n",
    "    for tuple in sent:\n",
    "        print(tuple)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to split the dataset in a 80% of the sentences for training, and 20% for test. All the sentences in these subsets will be concatenated in <code>train_words</code> and <code>test_words</code>, respectively. We are going to build a vocabulary <code>vocab</code> that will contain every word in the train set. The variable <code>tags</code> will contain the $L$ possible tags, which will be out states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into training and validation set in the ratio 80:20\n",
    "train_sentences,test_sentences =train_test_split(data,train_size=0.80,test_size=0.20,random_state = 101)\n",
    "\n",
    "# join all the sentences in train and test sets\n",
    "train_words = [ tup for sent in train_sentences for tup in sent ]\n",
    "test_words = [ tup for sent in test_sentences for tup in sent ]\n",
    "print('Train words: ' + str(len(train_words)))\n",
    "print('Test words: ' + str(len(test_words)))\n",
    "\n",
    "# check some of the tagged words.\n",
    "print('\\nFirst 20 training words: ')\n",
    "print(train_words[:20])\n",
    "\n",
    "# use set datatype to check how many unique tags are present in training data\n",
    "tags = {tag for word,tag in train_words}\n",
    "L = len(tags)\n",
    "print('\\nNumber of tags: ' + str(L))\n",
    "print(tags)\n",
    " \n",
    "# check total words in vocabulary\n",
    "vocab = {word for word,tag in train_words}\n",
    "print('\\nNumber of words in the vocabulary: ' + str(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.1. HMM with fully observed data\n",
    "\n",
    "\n",
    "**TASK3: Build the functions for obtaining the emission and transition probabilities, using expressions of section 2.3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def emission(word, tag, train_bag = train_words):\n",
    "    \"\"\"Probability B_im that a tag i emits a word m, computed from train_bag\"\"\"\n",
    "    # 1 Count the number of times the given word appears in the training set with the given tag, N_im\n",
    "    # 2 Count the number of words with the given tag in the training set, N_i\n",
    "    # B_im = N_im / N_i\n",
    "    \n",
    "    N_i = [t[1] for t in train_bag].count(tag)\n",
    "    # To avoid division by zero error\n",
    "    if N_i == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        N_im = [w[0] for w in train_bag].count(word)\n",
    "        \n",
    "        B_im = N_im/N_i\n",
    "        return B_im\n",
    "\n",
    "def transition(tag1, tag2, train_bag = train_words):\n",
    "    \"\"\"Probability A_ij of state j (tag2) given that previous state was i (tag1)\"\"\"\n",
    "    # 1 Obtain the tag sequence from training set\n",
    "    # 2 Count the number of times tag 1 occurs, N_i\n",
    "    # 3 Count the number of times tag_2 occurs after tag_1, N_ij\n",
    "    # A_ij = N_ij / N_i\n",
    "    # print(tag1,tag2)\n",
    "    N_i = [t[1] for t in train_bag].count(tag1)\n",
    "    N_ij = [t[1] for index,t in enumerate(train_bag) if train_bag[index-1][1] == tag1].count(tag2)\n",
    "    # print(N_ij,N_i)\n",
    "    A_ij = N_ij / N_i \n",
    "\n",
    "    return A_ij\n",
    "        \n",
    "def transition_matrix(tags, train_bag = train_words):\n",
    "    \"\"\" Build transition matrix A of dimensions LxL from train_bag\"\"\"\n",
    "    # A_ij is the transition probability from tag i to tag j\n",
    "    \n",
    "    tags = list(tags) #SET Objects are not iterable\n",
    "    L = len(tags)\n",
    "    A = np.zeros((L,L))\n",
    "    for r in range(L):\n",
    "        for c in range(L):\n",
    "            A[r,c] = transition(tags[r],tags[c])\n",
    "    \n",
    "    return A\n",
    "\n",
    "def Viterbi(words, transition_matrix, train_bag = train_words):\n",
    "    \"\"\" Obtain the most probable sequence of tags given a list of words\"\"\"\n",
    "    state = []\n",
    "    T = list(set([pair[1] for pair in train_bag]))\n",
    "     \n",
    "    for key, word in enumerate(words):\n",
    "        #initialise list of probability column for a given observation\n",
    "        p = [] \n",
    "        for tag in T:\n",
    "            if key == 0:\n",
    "                transition_p = transition_matrix.loc['.', tag]\n",
    "            else:\n",
    "                transition_p = transition_matrix.loc[state[-1], tag]\n",
    "                 \n",
    "            # compute emission and state probabilities\n",
    "            emission_p = emission(words[key], tag)\n",
    "            state_probability = emission_p * transition_p    \n",
    "            p.append(state_probability)\n",
    "             \n",
    "        pmax = max(p)\n",
    "        # getting state for which probability is maximum\n",
    "        state_max = T[p.index(pmax)] \n",
    "        state.append(state_max)\n",
    "        \n",
    "    return list(zip(words, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the matrix to a df for better readability\n",
    "#the table is same as the transition table shown in section 3 of article\n",
    "A = transition_matrix(tags)\n",
    "A = pd.DataFrame(A, columns = list(tags), index=list(tags))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(A)\n",
    "plt.xticks(np.arange(len(tags)), list(tags))\n",
    "plt.yticks(np.arange(len(tags)), list(tags))\n",
    "display(A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TASK4: Choosing 10 random sequences from the test set, use the Viterbi function to estimate their tags. Compute the accuracy as the proportion of corrected tagged words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:33<00:00,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for the random sentence 0 is: 0.0\n",
      "The accuracy for the random sentence 1 is: 0.045454545454545456\n",
      "The accuracy for the random sentence 2 is: 0.16666666666666666\n",
      "The accuracy for the random sentence 3 is: 0.07142857142857142\n",
      "The accuracy for the random sentence 4 is: 0.08571428571428572\n",
      "The accuracy for the random sentence 5 is: 0.2\n",
      "The accuracy for the random sentence 6 is: 0.06666666666666667\n",
      "The accuracy for the random sentence 7 is: 0.0\n",
      "The accuracy for the random sentence 8 is: 0.16129032258064516\n",
      "The accuracy for the random sentence 9 is: 0.11764705882352941\n",
      "The average accuracy for this 10 random sentences is: 0.09148681173349105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "random.seed(4396)\n",
    "\n",
    "accuracies = []\n",
    "randomSeq = [test_sentences[random.randint(0,len(test_sentences) - 1)] for _ in range(10)]\n",
    "for sent in tqdm(randomSeq):\n",
    "    words = [w[0] for w in sent]\n",
    "    randomTags = [t[1] for t in sent]\n",
    "    viterbiTags = [t[1] for t in Viterbi(words,A)] \n",
    "    totalTags = list(zip(randomTags,viterbiTags))\n",
    "    accuracies.append(sum(r==v for r,v in totalTags)/len(sent)) \n",
    "\n",
    "for i,value in enumerate(accuracies):\n",
    "    print(f'The accuracy for the random sentence {i} is: {value}')\n",
    "\n",
    "print(f'The average accuracy for this 10 random sentences is: {np.mean(accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2. Sampling sentences\n",
    "\n",
    "We know how to obtain the parameters of our model when we have fully observed data. As we have a generative model, we can obtain samples from it. In this section, you will build a Multinomial HMM object of hmmlearn and sample sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK5: Obtain the emission matrix $\\mathbf{B}$ matrix using the emission function. You should calculate the probability of each word in <code>vocab</code> for each tag.**\n",
    "\n",
    "*Note: $\\mathbf{B}$ will be a huge matrix. This computation might take a while (up to half an hour).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.35426282e-05 4.35426282e-05 4.35426282e-05 ... 4.35426282e-05\n",
      "  4.35426282e-05 4.35426282e-05]\n",
      " [9.20810313e-05 9.20810313e-05 9.20810313e-05 ... 9.20810313e-05\n",
      "  9.20810313e-05 9.20810313e-05]\n",
      " [1.26550240e-04 1.26550240e-04 1.26550240e-04 ... 1.26550240e-04\n",
      "  1.26550240e-04 1.26550240e-04]\n",
      " ...\n",
      " [5.48847420e-04 5.48847420e-04 5.48847420e-04 ... 5.48847420e-04\n",
      "  5.48847420e-04 5.48847420e-04]\n",
      " [3.91389432e-04 3.91389432e-04 3.91389432e-04 ... 3.91389432e-04\n",
      "  3.91389432e-04 3.91389432e-04]\n",
      " [3.87897595e-04 3.87897595e-04 3.87897595e-04 ... 3.87897595e-04\n",
      "  3.87897595e-04 3.87897595e-04]]\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "def emission_matrix(train_bag=train_words):\n",
    "    N_im = [w[0] for w in train_bag]\n",
    "    N_i = [t[1] for t in train_bag]\n",
    "    \n",
    "    k = {}\n",
    "    for j in N_i:\n",
    "        if j in k:\n",
    "            k[j] += 1\n",
    "        else:\n",
    "            k[j] = 1\n",
    "    \n",
    "    h = {}\n",
    "    for i in N_im:\n",
    "        if i in h:\n",
    "            h[i] += 1\n",
    "        else:\n",
    "            h[i] = 1\n",
    "            \n",
    "    B = np.zeros((len(k), len(h)))\n",
    "\n",
    "    for i, ct in enumerate(k.values()):\n",
    "        for j, cw in enumerate(h.values()):\n",
    "            B[i,j] = cw/ct\n",
    "            \n",
    "    return B\n",
    "\n",
    "B = emission_matrix()\n",
    "\n",
    "#print(f\"{B.shape=}\")\n",
    "\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK6: Compute the parameters $\\boldsymbol{\\pi}$ as the proportion of the tags in the train set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=array([0.28596688, 0.135226  , 0.09839372, 0.06412651, 0.11606276,\n",
      "       0.03487735, 0.06478645, 0.02733159, 0.08662682, 0.02268709,\n",
      "       0.03181422, 0.03210061])\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "def pi_prop(tags=tags, train_bag=train_words):\n",
    "    N_i = [t[1] for t in train_bag]\n",
    "    total_sum = 0\n",
    "    k = {}\n",
    "    for j in N_i:\n",
    "        total_sum += 1\n",
    "        if j in k:\n",
    "            k[j] += 1\n",
    "        else:\n",
    "            k[j] = 1\n",
    "    pi = np.zeros(len(k))\n",
    "    \n",
    "    for i, ct in enumerate(k.values()):\n",
    "        pi[i] = ct/total_sum\n",
    "    \n",
    "    return pi\n",
    "            \n",
    "a = pi_prop(tags, train_words)\n",
    "\n",
    "print(f\"{a=}\")\n",
    "\n",
    "print(sum(a)) # Should sum up to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK7: Build a Multinomial HMM object with the computed parameters and print 30 words sampled from it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmmlearn.hmm import MultinomialHMM\n",
    "\n",
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "\n",
    "multimodel = MultinomialHMM(n_components=L)\n",
    "multimodel.startprob_ = pi_prop(tags, train_words)\n",
    "\n",
    "multimodel.transmat_ = transition_matrix(tags)\n",
    "\n",
    "multimodel.emissionprob_ = emission_matrix(train_words)\n",
    "\n",
    "Y, S = multimodel.sample(30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drink had had Drink Competes Competes Competes Competes Carrier Drink count Candela count , Candela Cartons count Carrier last Competes Carrier Cartons With last , Drink last Carrier , Drink "
     ]
    }
   ],
   "source": [
    "# print(f\"{Y=}\")\n",
    "# print(f\"{S=}\")\n",
    "\n",
    "words, tags = zip(*train_words)\n",
    "\n",
    "#y_list = Y.tolist()\n",
    "y_list = list(np.concatenate(Y).flat)\n",
    "s_list = S.tolist()\n",
    "\n",
    "for i in s_list:\n",
    "    print(words[i], end=\" \")\n",
    "    \n",
    "# print(y_list)\n",
    "# print(s_list)\n",
    "\n",
    "# tags_list = list(tags)\n",
    "# words_list = list(words)\n",
    "\n",
    "# # for y in y_list:\n",
    "# #     for s in s_list:\n",
    "# #         #print(words_list[y])\n",
    "        \n",
    "# print(len(train_words))\n",
    "\n",
    "# print(len(y_list))\n",
    "# print(len(s_list))\n",
    "# # for i, ct in enumerate(k.values()):\n",
    "# #     for j, cw in enumerate(h.values()):\n",
    "        \n",
    "        \n",
    "# #             N_im = [w[0] for w in train_bag]\n",
    "# #     N_i = [t[1] for t in train_bag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Q5. Do the sampled sequences follow a correct grammatical structure?**\n",
    "- Your answer.\n",
    "\n",
    "**Q6. Do the sampled sequences have a properly semantic content? Why?**\n",
    "- Your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1]. Murphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT press.\n",
    "\n",
    "[2]. Bishop, C. M. (2006). Pattern recognition and machine learning. springer.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "lab_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Massive_Computing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a8d75b773ac4dc6b053cb8dd8687f7f0c73faa24f39e165aefd390999d14b05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
